import cohere
import streamlit as st
from mcp.message_dispatcher import MCPMessage

# ⚠️ Always use environment variables or secrets manager in production
co = cohere.Client("2xTRZVraYBDx8GILR5d8CuTtwvyWnsiHayzM1TH5")

class LLMResponseAgent:
    def __init__(self, dispatcher):
        self.dispatcher = dispatcher

    def handle(self, message: MCPMessage):
        trace_id = message.trace_id
        print(f"[LLMResponseAgent] Received query with trace_id: {trace_id}")

        context = message.payload.get("retrieved_context") or message.payload.get("context", "")
        query = message.payload.get("query", "")

        if not context or not query:
            error = "❌ Missing context or query."
            print(f"[LLMResponseAgent] {error}")
            st.session_state["llm_response"] = error
            return

        # Prompt format - structured for clarity
        prompt = (
            "You are a helpful assistant. Use the following context to answer the question.\n\n"
            f"Context:\n{context.strip()}\n\n"
            f"Question: {query.strip()}\n\n"
            "Answer:"
        )

        try:
            response = co.generate(
                model="command-r-plus",  # "command" if using free tier
                prompt=prompt,
                max_tokens=300,
                temperature=0.3,
                stop_sequences=["--END--"]
            )

            if response.generations and response.generations[0].text.strip():
                answer = response.generations[0].text.strip()
                print(f"[LLMResponseAgent] Answer generated:\n{answer}")
                st.session_state["llm_response"] = answer
            else:
                warning = "⚠️ No response generated by the LLM."
                print(f"[LLMResponseAgent] {warning}")
                st.session_state["llm_response"] = warning

        except Exception as e:
            error_msg = f"[LLMResponseAgent] Error during LLM call: {e}"
            print(error_msg)
            st.session_state["llm_response"] = f"❌ LLM Error: {e}"

# ✅ Exportable handler for main.py
def llm_response_agent_handler(dispatcher):
    return LLMResponseAgent(dispatcher).handle
